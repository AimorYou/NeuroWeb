Unnamed: 0,project_id,commit_id,commit_message,files_changed,lines_inserted,lines_deleted,diff
0,155,edd6ebce,"Merge branch 'master' into 'update-package-versions-2'

# Conflicts:
#   README.md",9,224,180,"[{'new_path': 'FFMPEGFrames.py', 'diff': '@@ -14,7 +14,7 @@ class FFMPEGFrames:\n             os.makedirs(self.output)\n \n         query = ""ffmpeg -i "" + input + "" -vf fps="" + \\\n-            str(fps) + "" "" + self.output + ""output%02d.png""\n+            str(fps) + "" "" + self.output + ""/output%02d.png""\n         response = subprocess.Popen(\n             query, shell=True, stdout=subprocess.PIPE).stdout.read()\n         s = str(response).encode(\'utf-8\')\n'}, {'new_path': 'README.md', 'diff': '@@ -13,3 +13,6 @@ To run this script, you need to:\n     ```\n     This command will convert video into the series of images with 1 frame per second.\n     After that, array of images are going to be parsed into .txt file as text.\n+    ```bash\n+        python3 main.py -a files/videoplayback.mp4 output.txt\n+    ```\n'}, {'new_path': 'SpeechRecognition.py', 'diff': '@@ -1,4 +1,3 @@\n-\n # coding: utf-8\n \n # ### Функции пред- и постобработки\n@@ -10,37 +9,45 @@ from pydub import AudioSegment\n from scipy.io import wavfile\n import os\n import numpy as np\n+import speech_recognition as sr\n+import argparse\n+\n \n \n+parser = argparse.ArgumentParser(description=\'wav_to_txt\')\n+parser.add_argument(\'wav_path\', help=\'Path to .wav audio file\')\n+parser.add_argument(\'out_path\', help=\'Path to output .txt file\')\n # Делим аудио (wav) на короткие части, сохраняем в папку tmp_slices\n \n # In[2]:\n \n \n+\n+\n def divide_audio(audio_src, slice_length_ms=3000, overlap_ms=500):\n     audio = AudioSegment.from_file(audio_src, ""wav"")\n \n-    slices = np.arange(0, len(audio)-overlap_ms, slice_length_ms-overlap_ms)\n-    \n+    slices = np.arange(0, len(audio) - overlap_ms, slice_length_ms - overlap_ms)\n+\n     path = ""tmp_slices""\n     os.mkdir(path)\n     audio_slices_src = []\n     i = 0\n-    \n+\n     for start, end in zip(slices[:-1], slices[1:]):\n-        slice_ = audio[start : end+overlap_ms]\n+        slice_ = audio[start: end + overlap_ms]\n         # сохраняем кусочки аудио\n         slice_name = ""tmp_slices\\slice{0}.wav"".format(i)\n         audio_slices_src.append(slice_name)\n         slice_.export(slice_name, format=""wav"")\n-        i+=1\n-        \n+        i += 1\n+\n     if len(audio) - end > overlap_ms:\n-        slice_ = audio[end : len(audio)]\n+        slice_ = audio[end: len(audio)]\n         slice_name = ""tmp_slices\\slice{0}.wav"".format(i)\n         audio_slices_src.append(slice_name)\n         slice_.export(slice_name, format=""wav"")\n-        \n+\n     return audio_slices_src\n \n \n@@ -62,27 +69,27 @@ def delete_tmp_slices(audio_slices_src):\n \n def combine_text(text_array):\n     result_text = \'\'\n-    \n+\n     prev_splited = []\n-    \n+\n     for text in text_array:\n         processed_text = \'\'\n-        \n+\n         splited = text.lower().split()\n-        \n+\n         j = 0\n         while j < min(len(splited), len(prev_splited)):\n-            if splited[j] != prev_splited[-1-j]:\n+            if splited[j] != prev_splited[-1 - j]:\n                 break\n             j += 1\n-        \n+\n         for i in range(j, len(splited)):\n             processed_text += splited[i] + \' \'\n-        \n+\n         result_text += processed_text\n-        \n-        prev_splited = splited        \n-        \n+\n+        prev_splited = splited\n+\n     return result_text\n \n \n@@ -99,9 +106,6 @@ def combine_text(text_array):\n # In[5]:\n \n \n-import speech_recognition as sr\n-\n-\n # Распознавание без учета шума\n \n # In[6]:\n@@ -109,35 +113,35 @@ import speech_recognition as sr\n \n def recognize_no_noise(audio_src):\n     r = sr.Recognizer()\n-    \n+\n     # делим аудио на части\n     chunks_src = divide_audio(audio_src)\n     # распознаем каждую часть\n     text_array = []\n     for chunk_src in chunks_src:\n         a = sr.AudioFile(chunk_src)\n-        #print(\'analyzing \', chunk_src)\n+        # print(\'analyzing \', chunk_src)\n         with a as source:\n             audio = r.record(source)\n         try:\n             text_chunk = r.recognize_google(audio, language=\'ru\')\n             text_array.append(text_chunk)\n-            #print(\'!\',text_chunk)\n+            # print(\'!\',text_chunk)\n         except:\n             pass\n     # объединяем распознанные тексты\n     text = combine_text(text_array)\n-    \n+\n     # удаляем ненужные файлы\n     delete_tmp_slices(chunks_src)\n-    \n+\n     return text\n \n \n # In[7]:\n \n \n-#print(recognize_no_noise(\'audio/test1.wav\'))\n+# print(recognize_no_noise(\'audio/test1.wav\'))\n \n \n # Распознавание с учетом шума: уровень шума определяется автоматически\n@@ -147,35 +151,35 @@ def recognize_no_noise(audio_src):\n \n def recognize_with_noise(audio_src):\n     r = sr.Recognizer()\n-    \n+\n     # делим аудио на части\n     chunks_src = divide_audio(audio_src)\n     # распознаем каждую часть\n     text_array = []\n     for chunk_src in chunks_src:\n         a = sr.AudioFile(chunk_src)\n-        #print(\'analyzing \', chunk_src)\n+        # print(\'analyzing \', chunk_src)\n         with a as source:\n-            r.adjust_for_ambient_noise(source) # учитываем шум\n+            r.adjust_for_ambient_noise(source)  # учитываем шум\n             audio = r.record(source)\n         try:\n             text_chunk = r.recognize_google(audio, language=\'ru\')\n             text_array.append(text_chunk)\n-            #print(text_chunk)\n+            # print(text_chunk)\n         except:\n             pass\n     # объединяем распознанные тексты\n     text = combine_text(text_array)\n     # удаляем ненужные файлы\n     delete_tmp_slices(chunks_src)\n-    \n+\n     return text\n \n \n # In[9]:\n \n \n-#print(recognize_with_noise(\'audio/test1.wav\'))\n+# print(recognize_with_noise(\'audio/test1.wav\'))\n \n \n # Итоговая функция распознавания, в которой выбирается наилучший вариант распознавания\n@@ -184,18 +188,19 @@ def recognize_with_noise(audio_src):\n \n \n def wav_to_txt(audio_src, out_txt_src):\n-    \n+    print(f""Speech recognition started"")\n     no_noise_txt = recognize_no_noise(audio_src)\n     noise_txt = recognize_with_noise(audio_src)\n-    \n+\n     text = no_noise_txt\n     if len(noise_txt.split()) > len(text.split()):\n         text = noise_txt\n-        \n+\n     # записываем в файл\n     out = open(out_txt_src, \'a\')\n     out.write(text)\n     out.close()\n+    print(f""Speech recognition finished"")\n \n \n # Для интеграции в main\n@@ -203,17 +208,12 @@ def wav_to_txt(audio_src, out_txt_src):\n # In[12]:\n \n \n-#wav_to_txt(\'audio/test1.wav\', \'TEXT.txt\')\n+# wav_to_txt(\'audio/test1.wav\', \'TEXT.txt\')\n \n \n # In[ ]:\n \n \n-parser = argparse.ArgumentParser(description=\'wav_to_txt\')\n-parser.add_argument(\'wav_path\', help=\'Path to .wav audio file\')\n-parser.add_argument(\'out_path\', help=\'Path to output .txt file\')\n-\n if __name__ == \'__main__\':\n     args = parser.parse_args()\n     wav_to_txt(args.wav_path, args.out_path)\n-\n'}, {'new_path': 'SpeechRecognition_mp3_to_text.py', 'diff': '@@ -1,132 +0,0 @@\n-<<<<<<< HEAD:SpeechRecognition_2_mp3_to_text.py\n-# -*- coding: utf-8 -*-\n-=======\n-# coding: utf-8\n->>>>>>> origin/KateShestakova:SpeechRecognition_mp3_to_text.py\n-\n-# ### Функции пред- и постобработки\n-\n-from pydub import AudioSegment\n-from pydub.utils import make_chunks\n-import os\n-\n-import speech_recognition as sr\n-\n-# Перевод из формата mp3 в wav\n-\n-def mp3_to_wav(mp3_src, wav_src):\n-    sound = AudioSegment.from_mp3(mp3_src)\n-    sound.export(wav_src, format=""wav"")\n-\n-# Делим аудио (wav) на короткие части, сохраняем в папку tmp_chunks\n-\n-def divide_audio(audio_src, chunk_length_ms):\n-    audio = AudioSegment.from_file(audio_src, ""wav"") \n-    chunks = make_chunks(audio, chunk_length_ms)\n-    \n-    path = ""tmp_chunks""\n-    os.mkdir(path)\n-    \n-    chunks_src = []\n-    for i, chunk in enumerate(chunks):\n-        chunk_name = ""tmp_chunks\\chunk{0}.wav"".format(i)\n-        chunks_src.append(chunk_name)\n-        chunk.export(chunk_name, format=""wav"")\n-        \n-    return chunks_src\n-\n-\n-# Удаляем папку tmp_chunks с частями аудио\n-\n-def delete_tmp_chunks(chunks_src):\n-    for chunk in chunks_src:\n-        os.remove(chunk)\n-    os.rmdir(\'tmp_chunks\')\n-\n-# Соединяем распознанный по частям текст\n-\n-def combine_text(text_array):\n-    result_text = \'\'\n-    for text in text_array:\n-        result_text += text + \' \'\n-    return result_text\n-\n-\n-# ### Распознавание речи\n-\n-# Библиотека SpeechRecognition ( https://pypi.org/project/SpeechRecognition/ )\n-# \n-# Используется распознавание при помощи Google Speech Recognition\n-# \n-# Распознавание текста происходит непрерывно, даже когда речи нет, а есть только шум. Если распознать какие-то слова не удается, программа выдает ошибку. Поэтому нужно делить аудио на короткие части и распознавать их отдельно.\n-# \n-# Список языковых кодов для Google Speech Recognition: https://www.science.co.il/language/Locale-codes.php\n-\n-\n-# Распознавание без учета шума\n-\n-def recognize(audio_src):\n-    # делим аудио на части\n-    chunks_src = divide_audio(audio_src, 3000)\n-    # распознаем каждую часть\n-    text_array = []\n-    for chunk_src in chunks_src:\n-        a = sr.AudioFile(chunk_src)\n-        with a as source:\n-            audio = r.record(source)\n-        try:\n-            text_array.append(r.recognize_google(audio, language=\'ru\'))\n-        except:\n-            pass\n-    # объединяем распознанные тексты\n-    text = combine_text(text_array)\n-    # удаляем ненужные файлы\n-    delete_tmp_chunks(chunks_src)\n-    \n-    return text\n-\n-# Распознавание с учетом шума: уровень шума определяется автоматически\n-\n-def recognize_with_noise(audio_src):\n-    # делим аудио на части\n-    chunks_src = divide_audio(audio_src, 3000)\n-    # распознаем каждую часть\n-    text_array = []\n-    for chunk_src in chunks_src:\n-        a = sr.AudioFile(chunk_src)\n-        with a as source:\n-            r.adjust_for_ambient_noise(source) # учитываем шум\n-            audio = r.record(source)\n-        try:\n-            text_chunk = r.recognize_google(audio, language=\'ru\')\n-            text_array.append(text_chunk)\n-            #print(text_chunk)\n-        except:\n-            pass\n-    # объединяем распознанные тексты\n-    text = combine_text(text_array)\n-    # удаляем ненужные файлы\n-    delete_tmp_chunks(chunks_src)\n-    \n-    return text\n-\n-# итоговая функция распознавания\n-\n-def wav_to_txt(path):\n-    r = sr.Recognizer()\n-    recognize_with_noise(path)\n-    \n-# для интеграции в main\n-\n-parser = argparse.ArgumentParser(description=\'wav_to_txt\')\n-parser.add_argument(\'wav_path\', help=\'Path to .wav audio file\')\n-parser.add_argument(\'out_path\', help=\'Path to output text file\')\n-\n-if __name__ == \'__main__\':\n-    args = parser.parse_args()\n-    txt = wav_to_txt(args.wav_path)\n-    \n-    f = open(args.out_path, ""a"")\n-    f.write(txt) \n-    f.close() \n-\n'}, {'new_path': 'TextFromPicture.py', 'diff': ""@@ -6,6 +6,7 @@ import sys\n import os \n import argparse\n \n+\n parser = argparse.ArgumentParser(description='OCR')\n parser.add_argument('in_filenames', help='Input filenames')\n parser.add_argument('out_filename', help='Output filename')\n@@ -16,8 +17,8 @@ def Pic2Txt(listImg, outfile):\n \t# Iterate through all the image\n \tfor img in listImg: \n \n-\t\t# Recognize the text as string in image using pytesserct \n-\t\ttext = str(((pytesseract.image_to_string(cv2.imread(img),lang = 'eng+rus')))) \n+\t\t# Recognize the text as string in image using pytesserct\n+\t\ttext = str(((pytesseract.image_to_string(cv2.imread(img),lang = 'eng+rus'))))\n \n \t\t# The recognized text is stored in variable text \n \t\t# Any string processing may be applied on text \n""}, {'new_path': 'main.py', 'diff': '@@ -7,7 +7,8 @@ import socket\n from TextFromPicture import Pic2Txt\n import FFMPEGFrames\n import glob\n-\n+from SpeechRecognition import wav_to_txt\n+import video_to_audio\n \n def main():\n     parser = argparse.ArgumentParser()\n@@ -43,7 +44,20 @@ def main():\n \n         f = FFMPEGFrames.FFMPEGFrames(""images/"")\n         f.extract_frames(input, fps)\n-        Pic2Txt(glob.glob(""images/*.png""), output)\n+        Pic2Txt(glob.glob(f.output + ""/*.png""), output)\n+    elif str(sys.argv[1] in [\'-a\', \'--audio\']):\n+        parser.add_argument(\'-a\', \'--audio\', action=\'store_true\')\n+        parser.add_argument(""in_filename"", help=\'Input filename\')\n+        parser.add_argument(\'out_filename\', help=\'Output filename\')\n+        args = vars(parser.parse_args())\n+\n+        input = args[""in_filename""]\n+        output = args[""out_filename""]\n+\n+        f = video_to_audio.video_to_audio(""/WAVs"")\n+        f.video_to_wav(input)\n+        wav_to_txt(f.output, output)\n+\n     else:\n         print(error_msg)\n \n'}, {'new_path': 'mp3_to_wav.py', 'diff': '@@ -0,0 +1,37 @@\n+import subprocess\n+import argparse\n+import os\n+\n+parser = argparse.ArgumentParser(description=\'mp3_to_wav\')\n+parser.add_argument(\'in_mp3\', help=\'Input .mp3 file\')\n+\n+def mp3_to_wav(mp3_path):\n+    wavs_path = ""WAVs""\n+    wav_path = wavs_path + ""/output.wav""\n+    if not os.path.exists(wavs_path):\n+        os.mkdir(wavs_path)\n+\n+    subprocess.run([""ffmpeg"",\n+                ""-loglevel"",\n+                ""quiet"",\n+                ""-hide_banner"",\n+                ""-y"",\n+                ""-i"",\n+                mp3_path,\n+                ""-write_id3v1"",\n+                ""1"",\n+                ""-id3v2_version"",\n+                ""3"",\n+                ""-q:a"",\n+                ""0"",\n+                ""-map"",\n+                ""a"",\n+                wav_path],\n+               stderr=subprocess.DEVNULL,\n+               stdout=subprocess.DEVNULL,\n+               stdin=subprocess.PIPE)\n+\n+\n+if __name__ == \'__main__\':\n+    args = parser.parse_args()\n+    mp3_to_wav(args.in_mp3)\n\\ No newline at end of file\n'}, {'new_path': 'video2mp3.py', 'diff': '@@ -0,0 +1,40 @@\n+import subprocess\n+import argparse\n+import os\n+\n+parser = argparse.ArgumentParser(description=\'video2mp3\')\n+parser.add_argument(\'video_path\', help=\'Path to video file\')\n+\n+\n+def video2mp3(video_path):\n+    mp3s_path = ""mp3s""\n+    mp3_path = mp3s_path + ""/output.mp3""\n+    if not os.path.exists(mp3s_path):\n+        os.mkdir(mp3s_path)\n+\n+    subprocess.run([""ffmpeg"",\n+                ""-loglevel"",\n+                ""quiet"",\n+                ""-hide_banner"",\n+                ""-y"",\n+                ""-i"",\n+                video_path,\n+                ""-write_id3v1"",\n+                ""1"",\n+                ""-id3v2_version"",\n+                ""3"",\n+                ""-q:a"",\n+                ""0"",\n+                ""-map"",\n+                ""a"",\n+                mp3_path],\n+               stderr=subprocess.DEVNULL,\n+               stdout=subprocess.DEVNULL,\n+               stdin=subprocess.PIPE)\n+\n+\n+if __name__ == \'__main__\':\n+    #path = ""files/videoplayback.mp4""\n+    #video2mp3(path)\n+    args = parser.parse_args()\n+    video2mp3(args.video_path)\n'}, {'new_path': 'video_to_audio.py', 'diff': '@@ -0,0 +1,81 @@\n+import subprocess\n+import argparse\n+import os\n+import time\n+\n+parser = argparse.ArgumentParser(description=\'video_to_wav\')\n+parser.add_argument(\'video_path\', help=\'Path to video file\')\n+\n+\n+class video_to_audio:\n+\n+    def __init__(self, output):\n+        self.output = output\n+\n+    def video_to_wav(self, video_path):\n+        wavs_path = ""WAVs""\n+        name = video_path.split(\'/\')[-1].split(\'.\')[0]\n+        self.output = wavs_path + ""/"" + name + "".wav""\n+        if not os.path.exists(wavs_path):\n+            os.mkdir(wavs_path)\n+            print(f""Directory created"")\n+\n+        subprocess.run([""ffmpeg"",\n+                ""-loglevel"",\n+                ""debug"",\n+                ""-hide_banner"",\n+                ""-y"",\n+                ""-i"",\n+                video_path,\n+                ""-vn"",\n+                ""-sn"",\n+                ""-ar"",\n+                ""44100"",\n+                ""-q:a"",\n+                ""0"",\n+                ""-map"",\n+                ""a"",\n+                self.output],\n+               stderr=subprocess.DEVNULL,\n+               stdout=subprocess.DEVNULL,\n+               stdin=subprocess.PIPE)\n+\n+        print(f""Convertation to .wav finished!"")\n+\n+\n+#if __name__ == \'__main__\':\n+    #path = ""files/videoplayback.mp4""\n+    #video_to_wav(path)\n+    #args = parser.parse_args()\n+    #video_to_wav(args.video_path)\n+\n+\n+    def video2mp3(self, video_path):\n+        mp3s_path = ""mp3s""\n+        name = video_path.split(\'/\')[-1].split(\'.\')[0]\n+        self.output = mp3s_path + ""/"" + name + "".mp3""\n+        if not os.path.exists(mp3s_path):\n+            os.mkdir(mp3s_path)\n+            print(f""Directory created"")\n+\n+        subprocess.run([""ffmpeg"",\n+                ""-loglevel"",\n+                ""debug"",\n+                ""-hide_banner"",\n+                ""-y"",\n+                ""-i"",\n+                video_path,\n+                ""-write_id3v1"",\n+                ""1"",\n+                ""-id3v2_version"",\n+                ""3"",\n+                ""-q:a"",\n+                ""0"",\n+                ""-map"",\n+                ""a"",\n+                self.output],\n+               stderr=subprocess.DEVNULL,\n+               stdout=subprocess.DEVNULL,\n+               stdin=subprocess.PIPE)\n+\n+        print(f""Convertation to .mp3 finished!"")\n\\ No newline at end of file\n'}]"
1,155,8e92d98d,"updated package versions and first install script
",2,4,3,"[{'new_path': 'README.md', 'diff': '@@ -2,9 +2,9 @@\n \n To run this script, you need to:\n \n-1. Install ffmpeg, pytesseract, cv2\n+1. Install ffmpeg, pytesseract, cv2, pydub, scipy, SpeechRecognition\n     ```bash\n-        sudo pip3 install ffmpeg-python pytesseract opencv-python\n+        sudo pip3 install ffmpeg-python pytesseract opencv-python pydub scipy SpeechRecognition\n     ```\n 2. Put your video file into the folder ./files in the root of the project\n 3. Run from cli\n@@ -12,4 +12,4 @@ To run this script, you need to:\n         python3 main.py -v files/videoplayback.mp4 output.txt 1\n     ```\n     This command will convert video into the series of images with 1 frame per second.\n-    After that, array of images are going to be parsed into .txt file as text.\n\\ No newline at end of file\n+    After that, array of images are going to be parsed into .txt file as text.\n'}, {'new_path': 'requirements.txt', 'diff': '@@ -4,3 +4,4 @@ opencv-python==4.2.0.32\n pdf2image==1.12.1\n pydub==0.23.1\n pytesseract==0.3.3\n+scipy==1.4.1\n'}]"
2,155,cf4882df,"updated package versions and first install script
",5,62,189,"[{'new_path': 'README.md', 'diff': '@@ -4,7 +4,7 @@ To run this script, you need to:\n \n 1. Install ffmpeg, pytesseract, cv2\n     ```bash\n-        sudo pip3 install ffmpeg-python pytesseract opencv-python\n+        sudo pip3 install ffmpeg-python pytesseract opencv-python pydub scipy SpeechRecognition\n     ```\n 2. Put your video file into the folder ./files in the root of the project\n 3. Run from cli\n@@ -12,4 +12,4 @@ To run this script, you need to:\n         python3 main.py -v files/videoplayback.mp4 output.txt 1\n     ```\n     This command will convert video into the series of images with 1 frame per second.\n-    After that, array of images are going to be parsed into .txt file as text.\n\\ No newline at end of file\n+    After that, array of images are going to be parsed into .txt file as text.\n'}, {'new_path': 'SpeechRecognition.py', 'diff': '@@ -1,4 +1,3 @@\n-\n # coding: utf-8\n \n # ### Функции пред- и постобработки\n@@ -10,8 +9,12 @@ from pydub import AudioSegment\n from scipy.io import wavfile\n import os\n import numpy as np\n+import speech_recognition as sr\n+import argparse\n \n-\n+parser = argparse.ArgumentParser(description=""wav_to_txt"")\n+parser.add_argument(""wav_path"", help=""Path to .wav audio file"")\n+parser.add_argument(""out_path"", help=""Path to output .txt file"")\n # Делим аудио (wav) на короткие части, сохраняем в папку tmp_slices\n \n # In[2]:\n@@ -20,27 +23,27 @@ import numpy as np\n def divide_audio(audio_src, slice_length_ms=3000, overlap_ms=500):\n     audio = AudioSegment.from_file(audio_src, ""wav"")\n \n-    slices = np.arange(0, len(audio)-overlap_ms, slice_length_ms-overlap_ms)\n-    \n+    slices = np.arange(0, len(audio) - overlap_ms, slice_length_ms - overlap_ms)\n+\n     path = ""tmp_slices""\n     os.mkdir(path)\n     audio_slices_src = []\n     i = 0\n-    \n+\n     for start, end in zip(slices[:-1], slices[1:]):\n-        slice_ = audio[start : end+overlap_ms]\n+        slice_ = audio[start : end + overlap_ms]\n         # сохраняем кусочки аудио\n         slice_name = ""tmp_slices\\slice{0}.wav"".format(i)\n         audio_slices_src.append(slice_name)\n         slice_.export(slice_name, format=""wav"")\n-        i+=1\n-        \n+        i += 1\n+\n     if len(audio) - end > overlap_ms:\n         slice_ = audio[end : len(audio)]\n         slice_name = ""tmp_slices\\slice{0}.wav"".format(i)\n         audio_slices_src.append(slice_name)\n         slice_.export(slice_name, format=""wav"")\n-        \n+\n     return audio_slices_src\n \n \n@@ -52,7 +55,7 @@ def divide_audio(audio_src, slice_length_ms=3000, overlap_ms=500):\n def delete_tmp_slices(audio_slices_src):\n     for slice_ in audio_slices_src:\n         os.remove(slice_)\n-    os.rmdir(\'tmp_slices\')\n+    os.rmdir(""tmp_slices"")\n \n \n # Соединяем распознанный по частям текст\n@@ -61,47 +64,44 @@ def delete_tmp_slices(audio_slices_src):\n \n \n def combine_text(text_array):\n-    result_text = \'\'\n-    \n+    result_text = """"\n+\n     prev_splited = []\n-    \n+\n     for text in text_array:\n-        processed_text = \'\'\n-        \n+        processed_text = """"\n+\n         splited = text.lower().split()\n-        \n+\n         j = 0\n         while j < min(len(splited), len(prev_splited)):\n-            if splited[j] != prev_splited[-1-j]:\n+            if splited[j] != prev_splited[-1 - j]:\n                 break\n             j += 1\n-        \n+\n         for i in range(j, len(splited)):\n-            processed_text += splited[i] + \' \'\n-        \n+            processed_text += splited[i] + "" ""\n+\n         result_text += processed_text\n-        \n-        prev_splited = splited        \n-        \n+\n+        prev_splited = splited\n+\n     return result_text\n \n \n # ### Распознавание речи\n \n # Библиотека SpeechRecognition ( https://pypi.org/project/SpeechRecognition/ )\n-# \n+#\n # Используется распознавание при помощи Google Speech Recognition\n-# \n+#\n # Распознавание текста происходит непрерывно, даже когда речи нет, а есть только шум. Если распознать какие-то слова не удается, программа выдает ошибку. Поэтому нужно делить аудио на короткие части и распознавать их отдельно.\n-# \n+#\n # Список языковых кодов для Google Speech Recognition: https://www.science.co.il/language/Locale-codes.php\n \n # In[5]:\n \n \n-import speech_recognition as sr\n-\n-\n # Распознавание без учета шума\n \n # In[6]:\n@@ -109,35 +109,35 @@ import speech_recognition as sr\n \n def recognize_no_noise(audio_src):\n     r = sr.Recognizer()\n-    \n+\n     # делим аудио на части\n     chunks_src = divide_audio(audio_src)\n     # распознаем каждую часть\n     text_array = []\n     for chunk_src in chunks_src:\n         a = sr.AudioFile(chunk_src)\n-        #print(\'analyzing \', chunk_src)\n+        # print(\'analyzing \', chunk_src)\n         with a as source:\n             audio = r.record(source)\n         try:\n-            text_chunk = r.recognize_google(audio, language=\'ru\')\n+            text_chunk = r.recognize_google(audio, language=""ru"")\n             text_array.append(text_chunk)\n-            #print(\'!\',text_chunk)\n+            # print(\'!\',text_chunk)\n         except:\n             pass\n     # объединяем распознанные тексты\n     text = combine_text(text_array)\n-    \n+\n     # удаляем ненужные файлы\n     delete_tmp_slices(chunks_src)\n-    \n+\n     return text\n \n \n # In[7]:\n \n \n-#print(recognize_no_noise(\'audio/test1.wav\'))\n+# print(recognize_no_noise(\'audio/test1.wav\'))\n \n \n # Распознавание с учетом шума: уровень шума определяется автоматически\n@@ -147,35 +147,35 @@ def recognize_no_noise(audio_src):\n \n def recognize_with_noise(audio_src):\n     r = sr.Recognizer()\n-    \n+\n     # делим аудио на части\n     chunks_src = divide_audio(audio_src)\n     # распознаем каждую часть\n     text_array = []\n     for chunk_src in chunks_src:\n         a = sr.AudioFile(chunk_src)\n-        #print(\'analyzing \', chunk_src)\n+        # print(\'analyzing \', chunk_src)\n         with a as source:\n-            r.adjust_for_ambient_noise(source) # учитываем шум\n+            r.adjust_for_ambient_noise(source)  # учитываем шум\n             audio = r.record(source)\n         try:\n-            text_chunk = r.recognize_google(audio, language=\'ru\')\n+            text_chunk = r.recognize_google(audio, language=""ru"")\n             text_array.append(text_chunk)\n-            #print(text_chunk)\n+            # print(text_chunk)\n         except:\n             pass\n     # объединяем распознанные тексты\n     text = combine_text(text_array)\n     # удаляем ненужные файлы\n     delete_tmp_slices(chunks_src)\n-    \n+\n     return text\n \n \n # In[9]:\n \n \n-#print(recognize_with_noise(\'audio/test1.wav\'))\n+# print(recognize_with_noise(\'audio/test1.wav\'))\n \n \n # Итоговая функция распознавания, в которой выбирается наилучший вариант распознавания\n@@ -184,16 +184,15 @@ def recognize_with_noise(audio_src):\n \n \n def wav_to_txt(audio_src, out_txt_src):\n-    \n     no_noise_txt = recognize_no_noise(audio_src)\n     noise_txt = recognize_with_noise(audio_src)\n-    \n+\n     text = no_noise_txt\n     if len(noise_txt.split()) > len(text.split()):\n         text = noise_txt\n-        \n+\n     # записываем в файл\n-    out = open(out_txt_src, \'a\')\n+    out = open(out_txt_src, ""a"")\n     out.write(text)\n     out.close()\n \n@@ -203,17 +202,11 @@ def wav_to_txt(audio_src, out_txt_src):\n # In[12]:\n \n \n-#wav_to_txt(\'audio/test1.wav\', \'TEXT.txt\')\n+# wav_to_txt(\'audio/test1.wav\', \'TEXT.txt\')\n \n \n # In[ ]:\n \n-\n-parser = argparse.ArgumentParser(description=\'wav_to_txt\')\n-parser.add_argument(\'wav_path\', help=\'Path to .wav audio file\')\n-parser.add_argument(\'out_path\', help=\'Path to output .txt file\')\n-\n-if __name__ == \'__main__\':\n+if __name__ == ""__main__"":\n     args = parser.parse_args()\n     wav_to_txt(args.wav_path, args.out_path)\n-\n'}, {'new_path': 'SpeechRecognition_mp3_to_text.py', 'diff': '@@ -1,132 +0,0 @@\n-<<<<<<< HEAD:SpeechRecognition_2_mp3_to_text.py\n-# -*- coding: utf-8 -*-\n-=======\n-# coding: utf-8\n->>>>>>> origin/KateShestakova:SpeechRecognition_mp3_to_text.py\n-\n-# ### Функции пред- и постобработки\n-\n-from pydub import AudioSegment\n-from pydub.utils import make_chunks\n-import os\n-\n-import speech_recognition as sr\n-\n-# Перевод из формата mp3 в wav\n-\n-def mp3_to_wav(mp3_src, wav_src):\n-    sound = AudioSegment.from_mp3(mp3_src)\n-    sound.export(wav_src, format=""wav"")\n-\n-# Делим аудио (wav) на короткие части, сохраняем в папку tmp_chunks\n-\n-def divide_audio(audio_src, chunk_length_ms):\n-    audio = AudioSegment.from_file(audio_src, ""wav"") \n-    chunks = make_chunks(audio, chunk_length_ms)\n-    \n-    path = ""tmp_chunks""\n-    os.mkdir(path)\n-    \n-    chunks_src = []\n-    for i, chunk in enumerate(chunks):\n-        chunk_name = ""tmp_chunks\\chunk{0}.wav"".format(i)\n-        chunks_src.append(chunk_name)\n-        chunk.export(chunk_name, format=""wav"")\n-        \n-    return chunks_src\n-\n-\n-# Удаляем папку tmp_chunks с частями аудио\n-\n-def delete_tmp_chunks(chunks_src):\n-    for chunk in chunks_src:\n-        os.remove(chunk)\n-    os.rmdir(\'tmp_chunks\')\n-\n-# Соединяем распознанный по частям текст\n-\n-def combine_text(text_array):\n-    result_text = \'\'\n-    for text in text_array:\n-        result_text += text + \' \'\n-    return result_text\n-\n-\n-# ### Распознавание речи\n-\n-# Библиотека SpeechRecognition ( https://pypi.org/project/SpeechRecognition/ )\n-# \n-# Используется распознавание при помощи Google Speech Recognition\n-# \n-# Распознавание текста происходит непрерывно, даже когда речи нет, а есть только шум. Если распознать какие-то слова не удается, программа выдает ошибку. Поэтому нужно делить аудио на короткие части и распознавать их отдельно.\n-# \n-# Список языковых кодов для Google Speech Recognition: https://www.science.co.il/language/Locale-codes.php\n-\n-\n-# Распознавание без учета шума\n-\n-def recognize(audio_src):\n-    # делим аудио на части\n-    chunks_src = divide_audio(audio_src, 3000)\n-    # распознаем каждую часть\n-    text_array = []\n-    for chunk_src in chunks_src:\n-        a = sr.AudioFile(chunk_src)\n-        with a as source:\n-            audio = r.record(source)\n-        try:\n-            text_array.append(r.recognize_google(audio, language=\'ru\'))\n-        except:\n-            pass\n-    # объединяем распознанные тексты\n-    text = combine_text(text_array)\n-    # удаляем ненужные файлы\n-    delete_tmp_chunks(chunks_src)\n-    \n-    return text\n-\n-# Распознавание с учетом шума: уровень шума определяется автоматически\n-\n-def recognize_with_noise(audio_src):\n-    # делим аудио на части\n-    chunks_src = divide_audio(audio_src, 3000)\n-    # распознаем каждую часть\n-    text_array = []\n-    for chunk_src in chunks_src:\n-        a = sr.AudioFile(chunk_src)\n-        with a as source:\n-            r.adjust_for_ambient_noise(source) # учитываем шум\n-            audio = r.record(source)\n-        try:\n-            text_chunk = r.recognize_google(audio, language=\'ru\')\n-            text_array.append(text_chunk)\n-            #print(text_chunk)\n-        except:\n-            pass\n-    # объединяем распознанные тексты\n-    text = combine_text(text_array)\n-    # удаляем ненужные файлы\n-    delete_tmp_chunks(chunks_src)\n-    \n-    return text\n-\n-# итоговая функция распознавания\n-\n-def wav_to_txt(path):\n-    r = sr.Recognizer()\n-    recognize_with_noise(path)\n-    \n-# для интеграции в main\n-\n-parser = argparse.ArgumentParser(description=\'wav_to_txt\')\n-parser.add_argument(\'wav_path\', help=\'Path to .wav audio file\')\n-parser.add_argument(\'out_path\', help=\'Path to output text file\')\n-\n-if __name__ == \'__main__\':\n-    args = parser.parse_args()\n-    txt = wav_to_txt(args.wav_path)\n-    \n-    f = open(args.out_path, ""a"")\n-    f.write(txt) \n-    f.close() \n-\n'}, {'new_path': 'main.py', 'diff': '@@ -8,6 +8,7 @@ from TextFromPicture import Pic2Txt\n import FFMPEGFrames\n import glob\n \n+from SpeechRecognition import wav_to_txt\n \n def main():\n     parser = argparse.ArgumentParser()\n@@ -44,6 +45,16 @@ def main():\n         f = FFMPEGFrames.FFMPEGFrames(""images/"")\n         f.extract_frames(input, fps)\n         Pic2Txt(glob.glob(""images/*.png""), output)\n+    elif str(sys.argv[1] in [\'-a\', \'--audio\']):\n+        parser.add_argument(\'-a\', \'--audio\', action=\'store_true\')\n+        parser.add_argument(""in_filename"", help=\'Input filename\')\n+        parser.add_argument(\'out_filename\', help=\'Output filename\')\n+        args = vars(parser.parse_args())\n+\n+        input = args[""in_filename""]\n+        output = args[""out_filename""]\n+\n+        wav_to_txt(input, output)\n     else:\n         print(error_msg)\n \n'}, {'new_path': 'requirements.txt', 'diff': '@@ -3,4 +3,5 @@ ffmpeg-python==0.2.0\n opencv-python==4.2.0.32\n pdf2image==1.12.1\n pydub==0.23.1\n+scipy==1.4.1\n pytesseract==0.3.3\n'}]"
3,155,b30446a3,"Integrated video-to-frame module into the main.py
",4,16,7,"[{'new_path': '.gitignore', 'diff': '@@ -46,4 +46,7 @@ docs/_build/\n \n #output files\n output.txt\n-./images\n\\ No newline at end of file\n+./images\n+\n+# IDE folder\n+.vscode\n\\ No newline at end of file\n'}, {'new_path': 'FFMPEGFrames.py', 'diff': '@@ -1,16 +1,20 @@\n import os\n import subprocess\n \n+\n class FFMPEGFrames:\n+\n     def __init__(self, output):\n         self.output = output\n \n     def extract_frames(self, input, fps):\n-        output = input.split(\'/\')[-1].split(\'.\')[0]\n+        self.output = input.split(\'/\')[-1].split(\'.\')[0]\n \n         if not os.path.exists(self.output):\n             os.makedirs(self.output)\n \n-        query = ""ffmpeg -i "" + input + "" -vf fps="" + str(fps) + "" "" + self.output + ""output%02d.png""\n-        response = subprocess.Popen(query, shell=True, stdout=subprocess.PIPE).stdout.read()\n+        query = ""ffmpeg -i "" + input + "" -vf fps="" + \\\n+            str(fps) + "" "" + self.output + ""output%02d.png""\n+        response = subprocess.Popen(\n+            query, shell=True, stdout=subprocess.PIPE).stdout.read()\n         s = str(response).encode(\'utf-8\')\n'}, {'new_path': 'main.py', 'diff': '@@ -8,6 +8,7 @@ from TextFromPicture import Pic2Txt\n import FFMPEGFrames\n import glob\n \n+\n def main():\n     parser = argparse.ArgumentParser()\n \n@@ -18,7 +19,6 @@ def main():\n                 ""  -v, --video             Convert video to pictures and then convert to text\\n"" \\\n                 ""       Mandatory: --in_filename=\'files/video.mp4\' --out_filename=\'output.txt\'  --fps=\'1\'-  set input video, output file, frames per second ""\n \n-\n     if len(sys.argv) == 1:\n         print(error_msg)\n \n@@ -47,5 +47,6 @@ def main():\n     else:\n         print(error_msg)\n \n+\n if __name__ == \'__main__\':\n     main()\n'}, {'new_path': 'requirements.txt', 'diff': '@@ -1,5 +1,6 @@\n SpeechRecognition==3.8.1\n-ffmpeg==1.4\n+ffmpeg-python==0.2.0\n+opencv-python==4.2.0.32\n pdf2image==1.12.1\n pydub==0.23.1\n-pytesseract==0.3.2\n+pytesseract==0.3.3\n'}]"
4,155,88eb4ac7,"Integrated video-to-frame module into the main.py
",5,46,53,"[{'new_path': '.gitignore', 'diff': '@@ -43,3 +43,7 @@ coverage.xml\n \n # Sphinx documentation\n docs/_build/\n+\n+#output files\n+output.txt\n+./images\n\\ No newline at end of file\n'}, {'new_path': 'FFMPEGFrames.py', 'diff': '@@ -0,0 +1,16 @@\n+import os\n+import subprocess\n+\n+class FFMPEGFrames:\n+    def __init__(self, output):\n+        self.output = output\n+\n+    def extract_frames(self, input, fps):\n+        output = input.split(\'/\')[-1].split(\'.\')[0]\n+\n+        if not os.path.exists(self.output):\n+            os.makedirs(self.output)\n+\n+        query = ""ffmpeg -i "" + input + "" -vf fps="" + str(fps) + "" "" + self.output + ""output%02d.png""\n+        response = subprocess.Popen(query, shell=True, stdout=subprocess.PIPE).stdout.read()\n+        s = str(response).encode(\'utf-8\')\n'}, {'new_path': 'README.md', 'diff': '@@ -2,11 +2,14 @@\n \n To run this script, you need to:\n \n-1. Download ffmpeg for [Windows](https://ffmpeg.org/download.html#build-windows) or [MacOS](https://ffmpeg.org/download.html#build-mac\n-)\n-2. Put your video file into the folder ./videos in the root of the project\n+1. Install ffmpeg, pytesseract, cv2\n+    ```bash\n+        sudo pip3 install ffmpeg-python pytesseract opencv-python\n+    ```\n+2. Put your video file into the folder ./files in the root of the project\n 3. Run from cli\n-```bash\n-    ffmpeg -i videos/{video-name.format} -vf fps=1 images/out%d.png\n-```\n-This command will convert video into the series of images with 1 frame per second and put them info ./images folder\n\\ No newline at end of file\n+    ```bash\n+        python3 main.py -v files/videoplayback.mp4 output.txt 1\n+    ```\n+    This command will convert video into the series of images with 1 frame per second.\n+    After that, array of images are going to be parsed into .txt file as text.\n\\ No newline at end of file\n'}, {'new_path': 'get-video-stream.py', 'diff': ""@@ -1,35 +0,0 @@\n-#!/usr/bin/env python\n-from __future__ import unicode_literals, print_function\n-import argparse\n-import ffmpeg\n-import sys\n-\n-\n-parser = argparse.ArgumentParser(description='Generate video thumbnail')\n-parser.add_argument('in_filename', help='Input filename')\n-parser.add_argument('out_filename', help='Output filename')\n-parser.add_argument(\n-    '--time', type=int, default=0.1, help='Time offset')\n-parser.add_argument(\n-    '--width', type=int, default=120,\n-    help='Width of output thumbnail (height automatically determined by aspect ratio)')\n-\n-\n-def generate_thumbnail(in_filename, out_filename, time, width):\n-    try:\n-        (\n-            ffmpeg\n-            .input(in_filename, ss=time)\n-            .filter('scale', width, -1)\n-            .output(out_filename, vframes=1)\n-            .overwrite_output()\n-            .run(capture_stdout=True, capture_stderr=True)\n-        )\n-    except ffmpeg.Error as e:\n-        print(e.stderr.decode(), file=sys.stderr)\n-        sys.exit(1)\n-\n-\n-if __name__ == '__main__':\n-    args = parser.parse_args()\n-    generate_thumbnail(args.in_filename, args.out_filename, args.time, args.width)\n\\ No newline at end of file\n""}, {'new_path': 'main.py', 'diff': '@@ -4,8 +4,9 @@ from test import Hello\n import argparse\n import sys\n import socket\n-from  TextFromPicture import Pic2Txt\n-\n+from TextFromPicture import Pic2Txt\n+import FFMPEGFrames\n+import glob\n \n def main():\n     parser = argparse.ArgumentParser()\n@@ -14,8 +15,8 @@ def main():\n                 ""Commands: \\n"" \\\n                 ""  -t, --test                 Let\'s test it!\\n"" \\\n                 ""       Optional: --name=\'your_arg\'  -   that\'ll print your string your_arg\\n"" \\\n-                ""  -p, --pictures             Convert pictures to text\\n"" \\\n-                ""       Mandatory: --in_filenames=\'img1.jpg(optional:,img2.(format))\' --out_filename=\'output.txt\'  -  set input image(s) and output file ""\n+                ""  -v, --video             Convert video to pictures and then convert to text\\n"" \\\n+                ""       Mandatory: --in_filename=\'files/video.mp4\' --out_filename=\'output.txt\'  --fps=\'1\'-  set input video, output file, frames per second ""\n \n \n     if len(sys.argv) == 1:\n@@ -29,18 +30,22 @@ def main():\n \n         test_class = Hello(args.name)\n         test_class.print_hello()\n-    elif str(sys.argv[1]) in [\'-p\', \'--pictures\']:\n-        parser.add_argument(\'-p\', \'--pictures\', action=\'store_true\')\n-        parser.add_argument(\'in_filenames\', help=\'Input filenames\')\n+    elif str(sys.argv[1]) in [\'-v\', \'--video\']:\n+        parser.add_argument(\'-v\', \'--video\', action=\'store_true\')\n+        parser.add_argument(""in_filename"", help=\'Input filename\')\n         parser.add_argument(\'out_filename\', help=\'Output filename\')\n+        parser.add_argument(""fps"", help=\'fps\')\n+        args = vars(parser.parse_args())\n \n-        args = parser.parse_args()\n+        input = args[""in_filename""]\n+        output = args[""out_filename""]\n+        fps = args[""fps""]\n \n-        Pic2Txt(args.in_filenames.split(\',\'), args.out_filename)\n-        \n+        f = FFMPEGFrames.FFMPEGFrames(""images/"")\n+        f.extract_frames(input, fps)\n+        Pic2Txt(glob.glob(""images/*.png""), output)\n     else:\n         print(error_msg)\n \n-\n if __name__ == \'__main__\':\n     main()\n'}]"
5,155,dd6d6083,Delete SpeechRecognition_mp3_to_text.py - previous version,1,0,128,"[{'new_path': 'SpeechRecognition_mp3_to_text.py', 'diff': '@@ -1,128 +0,0 @@\n-# coding: utf-8\n-\n-# ### Функции пред- и постобработки\n-\n-from pydub import AudioSegment\n-from pydub.utils import make_chunks\n-import os\n-\n-import speech_recognition as sr\n-\n-# Перевод из формата mp3 в wav\n-\n-def mp3_to_wav(mp3_src, wav_src):\n-    sound = AudioSegment.from_mp3(mp3_src)\n-    sound.export(wav_src, format=""wav"")\n-\n-# Делим аудио (wav) на короткие части, сохраняем в папку tmp_chunks\n-\n-def divide_audio(audio_src, chunk_length_ms):\n-    audio = AudioSegment.from_file(audio_src, ""wav"") \n-    chunks = make_chunks(audio, chunk_length_ms)\n-    \n-    path = ""tmp_chunks""\n-    os.mkdir(path)\n-    \n-    chunks_src = []\n-    for i, chunk in enumerate(chunks):\n-        chunk_name = ""tmp_chunks\\chunk{0}.wav"".format(i)\n-        chunks_src.append(chunk_name)\n-        chunk.export(chunk_name, format=""wav"")\n-        \n-    return chunks_src\n-\n-\n-# Удаляем папку tmp_chunks с частями аудио\n-\n-def delete_tmp_chunks(chunks_src):\n-    for chunk in chunks_src:\n-        os.remove(chunk)\n-    os.rmdir(\'tmp_chunks\')\n-\n-# Соединяем распознанный по частям текст\n-\n-def combine_text(text_array):\n-    result_text = \'\'\n-    for text in text_array:\n-        result_text += text + \' \'\n-    return result_text\n-\n-\n-# ### Распознавание речи\n-\n-# Библиотека SpeechRecognition ( https://pypi.org/project/SpeechRecognition/ )\n-# \n-# Используется распознавание при помощи Google Speech Recognition\n-# \n-# Распознавание текста происходит непрерывно, даже когда речи нет, а есть только шум. Если распознать какие-то слова не удается, программа выдает ошибку. Поэтому нужно делить аудио на короткие части и распознавать их отдельно.\n-# \n-# Список языковых кодов для Google Speech Recognition: https://www.science.co.il/language/Locale-codes.php\n-\n-\n-# Распознавание без учета шума\n-\n-def recognize(audio_src):\n-    # делим аудио на части\n-    chunks_src = divide_audio(audio_src, 3000)\n-    # распознаем каждую часть\n-    text_array = []\n-    for chunk_src in chunks_src:\n-        a = sr.AudioFile(chunk_src)\n-        with a as source:\n-            audio = r.record(source)\n-        try:\n-            text_array.append(r.recognize_google(audio, language=\'ru\'))\n-        except:\n-            pass\n-    # объединяем распознанные тексты\n-    text = combine_text(text_array)\n-    # удаляем ненужные файлы\n-    delete_tmp_chunks(chunks_src)\n-    \n-    return text\n-\n-# Распознавание с учетом шума: уровень шума определяется автоматически\n-\n-def recognize_with_noise(audio_src):\n-    # делим аудио на части\n-    chunks_src = divide_audio(audio_src, 3000)\n-    # распознаем каждую часть\n-    text_array = []\n-    for chunk_src in chunks_src:\n-        a = sr.AudioFile(chunk_src)\n-        with a as source:\n-            r.adjust_for_ambient_noise(source) # учитываем шум\n-            audio = r.record(source)\n-        try:\n-            text_chunk = r.recognize_google(audio, language=\'ru\')\n-            text_array.append(text_chunk)\n-            #print(text_chunk)\n-        except:\n-            pass\n-    # объединяем распознанные тексты\n-    text = combine_text(text_array)\n-    # удаляем ненужные файлы\n-    delete_tmp_chunks(chunks_src)\n-    \n-    return text\n-\n-# итоговая функция распознавания\n-\n-def wav_to_txt(path):\n-    r = sr.Recognizer()\n-    recognize_with_noise(path)\n-    \n-# для интеграции в main\n-\n-parser = argparse.ArgumentParser(description=\'wav_to_txt\')\n-parser.add_argument(\'wav_path\', help=\'Path to .wav audio file\')\n-parser.add_argument(\'out_path\', help=\'Path to output text file\')\n-\n-if __name__ == \'__main__\':\n-    args = parser.parse_args()\n-    txt = wav_to_txt(args.wav_path)\n-    \n-    f = open(args.out_path, ""a"")\n-    f.write(txt) \n-    f.close() \n-\n'}]"
6,155,3a9348b9,Update SpeechRecognition_mp3_to_text.py,1,17,14,"[{'new_path': 'SpeechRecognition_mp3_to_text.py', 'diff': '@@ -14,9 +14,6 @@ def mp3_to_wav(mp3_src, wav_src):\n     sound = AudioSegment.from_mp3(mp3_src)\n     sound.export(wav_src, format=""wav"")\n \n-# mp3_to_wav(\'audio\\grob.mp3\', \'audio\\grob.wav\')\n-\n-\n # Делим аудио (wav) на короткие части, сохраняем в папку tmp_chunks\n \n def divide_audio(audio_src, chunk_length_ms):\n@@ -84,10 +81,6 @@ def recognize(audio_src):\n     \n     return text\n \n-\n-#print(recognize(\'audio/vsyo_idet_po_planu.wav\'))\n-\n-\n # Распознавание с учетом шума: уровень шума определяется автоматически\n \n def recognize_with_noise(audio_src):\n@@ -97,7 +90,6 @@ def recognize_with_noise(audio_src):\n     text_array = []\n     for chunk_src in chunks_src:\n         a = sr.AudioFile(chunk_src)\n-        #print(\'analyzing \', chunk_src)\n         with a as source:\n             r.adjust_for_ambient_noise(source) # учитываем шум\n             audio = r.record(source)\n@@ -114,12 +106,23 @@ def recognize_with_noise(audio_src):\n     \n     return text\n \n-#print(recognize_with_noise(\'audio/vsyo_idet_po_planu.wav\'))\n-\n # итоговая функция распознавания\n \n-def mp3_to_txt():\n-\n+def wav_to_txt(path):\n     r = sr.Recognizer()\n-    mp3_to_wav(\'audio/grob.mp3\', \'audio/grob.wav\')\n-    recognize_with_noise(\'audio/grob.wav\')\n+    recognize_with_noise(path)\n+    \n+# для интеграции в main\n+\n+parser = argparse.ArgumentParser(description=\'wav_to_txt\')\n+parser.add_argument(\'wav_path\', help=\'Path to .wav audio file\')\n+parser.add_argument(\'out_path\', help=\'Path to output text file\')\n+\n+if __name__ == \'__main__\':\n+    args = parser.parse_args()\n+    txt = wav_to_txt(args.wav_path)\n+    \n+    f = open(args.out_path, ""a"")\n+    f.write(txt) \n+    f.close() \n+\n'}]"
7,155,db983833,Update SpeechRecognition_2_mp3_to_text.py,1,0,1,"[{'new_path': 'SpeechRecognition_mp3_to_text.py', 'diff': '@@ -1,4 +1,3 @@\n-\n # coding: utf-8\n \n # ### Функции пред- и постобработки\n'}]"
8,155,3fe6883e,"TextFromPicture gets input parameters and called from main file with -p
arg
",2,48,80,"[{'new_path': 'TextFromPicture.py', 'diff': '@@ -1,85 +1,42 @@\n-# -*- coding: utf-8 -*-\n-\n # Import libraries\n # from PIL import Image хуже распознается\n import cv2 \n import pytesseract \n import sys \n-from pdf2image import convert_from_path \n import os \n-\n-# Path of the pdf \n-PDF_file = ""FileName.pdf""\n-\n-\'\'\' \n-Part #1 : Converting PDF to images \n-\'\'\'\n-\n-# Store all the pages of the PDF in a variable \n-pages = convert_from_path(PDF_file, 500) \n-\n-# Counter to store images of each page of PDF to image \n-image_counter = 1\n-\n-# Iterate through all the pages stored above \n-for page in pages: \n-\n-\t# Declaring filename for each page of PDF as JPG \n-\t# For each page, filename will be: \n-\t# PDF page 1 -> page_1.jpg \n-\t# PDF page 2 -> page_2.jpg \n-\t# PDF page 3 -> page_3.jpg \n-\t# .... \n-\t# PDF page n -> page_n.jpg \n-\tfilename = ""page_""+str(image_counter)+"".jpg""\n-\t\n-\t# Save the image of the page in system \n-\tpage.save(filename, \'JPEG\') \n-\n-\t# Increment the counter to update filename \n-\timage_counter = image_counter + 1\n-\n-\'\'\' \n-Part #2 - Recognizing text from the images using OCR \n-\'\'\'\n-# Variable to get count of total number of pages \n-filelimit = image_counter-1\n-\n-# Creating a text file to write the output \n-outfile = ""out_text.txt""\n-\n-# Open the file in append mode so that \n-# All contents of all images are added to the same file \n-f = open(outfile, ""a"") \n-\n-# Iterate from 1 to total number of pages \n-for i in range(1, filelimit + 1): \n-\n-\t# Set filename to recognize text from \n-\t# Again, these files will be: \n-\t# page_1.jpg \n-\t# page_2.jpg \n-\t# .... \n-\t# page_n.jpg \n-\tfilename = ""page_""+str(i)+"".jpg""\n-\t\t\n-\t# Recognize the text as string in image using pytesserct \n-\ttext = str(((pytesseract.image_to_string(cv2.imread(filename),lang = \'eng+rus\')))) \n-\n-\t# The recognized text is stored in variable text \n-\t# Any string processing may be applied on text \n-\t# Here, basic formatting has been done: \n-\t# In many PDFs, at line ending, if a word can\'t \n-\t# be written fully, a \'hyphen\' is added. \n-\t# The rest of the word is written in the next line \n-\t# Eg: This is a sample text this word here GeeksF- \n-\t# orGeeks is half on first line, remaining on next. \n-\t# To remove this, we replace every \'-\\n\' to \'\'. \n-\ttext = text.replace(\'-\\n\', \'\')\t \n-\n-\t# Finally, write the processed text to the file. \n-\t#with open(fname, ""w"", encoding=""utf-8"") as f:\n-\tf.write(text) \n-\n-# Close the file after writing all the text. \n-f.close() \n+import argparse\n+\n+parser = argparse.ArgumentParser(description=\'OCR\')\n+parser.add_argument(\'in_filenames\', help=\'Input filenames\')\n+parser.add_argument(\'out_filename\', help=\'Output filename\')\n+\n+def Pic2Txt(listImg, outfile):\n+\n+\tf = open(outfile, ""a"") \n+\t# Iterate through all the image\n+\tfor img in listImg: \n+\n+\t\t# Recognize the text as string in image using pytesserct \n+\t\ttext = str(((pytesseract.image_to_string(cv2.imread(img),lang = \'eng+rus\')))) \n+\n+\t\t# The recognized text is stored in variable text \n+\t\t# Any string processing may be applied on text \n+\t\t# Here, basic formatting has been done: \n+\t\t# In many PDFs, at line ending, if a word can\'t \n+\t\t# be written fully, a \'hyphen\' is added. \n+\t\t# The rest of the word is written in the next line \n+\t\t# Eg: This is a sample text this word here GeeksF- \n+\t\t# orGeeks is half on first line, remaining on next. \n+\t\t# To remove this, we replace every \'-\\n\' to \'\'. \n+\t\ttext = text.replace(\'-\\n\', \'\')\t \n+\n+\t\t# Finally, write the processed text to the file. \n+\t\t#with open(fname, ""w"", encoding=""utf-8"") as f:\n+\t\tf.write(text) \n+\n+\t# Close the file after writing all the text. \n+\tf.close() \n+\n+if __name__ == \'__main__\':\n+    args = parser.parse_args()\n+    Pic2Txt(args.in_filenames.split(\',\'), args.out_filename)\n\\ No newline at end of file\n'}, {'new_path': 'main.py', 'diff': '@@ -4,6 +4,7 @@ from test import Hello\n import argparse\n import sys\n import socket\n+from  TextFromPicture import Pic2Txt\n \n \n def main():\n@@ -12,7 +13,9 @@ def main():\n     error_msg = ""Invalid Arguments\\n\\n"" \\\n                 ""Commands: \\n"" \\\n                 ""  -t, --test                 Let\'s test it!\\n"" \\\n-                ""       Optional: --name=\'your_arg\'  -   that\'ll print your string your_arg""\n+                ""       Optional: --name=\'your_arg\'  -   that\'ll print your string your_arg\\n"" \\\n+                ""  -p, --pictures             Convert pictures to text\\n"" \\\n+                ""       Mandatory: --in_filenames=\'img1.jpg(optional:,img2.(format))\' --out_filename=\'output.txt\'  -  set input image(s) and output file ""\n \n \n     if len(sys.argv) == 1:\n@@ -26,7 +29,15 @@ def main():\n \n         test_class = Hello(args.name)\n         test_class.print_hello()\n+    elif str(sys.argv[1]) in [\'-p\', \'--pictures\']:\n+        parser.add_argument(\'-p\', \'--pictures\', action=\'store_true\')\n+        parser.add_argument(\'in_filenames\', help=\'Input filenames\')\n+        parser.add_argument(\'out_filename\', help=\'Output filename\')\n \n+        args = parser.parse_args()\n+\n+        Pic2Txt(args.in_filenames.split(\',\'), args.out_filename)\n+        \n     else:\n         print(error_msg)\n \n'}]"
9,155,15819229,"Merge remote-tracking branch 'origin/master' into SofiaKochemasova
",6,171,2,"[{'new_path': '.gitattributes', 'diff': '@@ -0,0 +1,2 @@\n+# Auto detect text files and perform LF normalization\n+* text=auto\n'}, {'new_path': 'SpeechRecognition_2_mp3_to_text.py', 'diff': '@@ -0,0 +1,125 @@\n+# -*- coding: utf-8 -*-\n+\n+# ### Функции пред- и постобработки\n+\n+from pydub import AudioSegment\n+from pydub.utils import make_chunks\n+import os\n+\n+import speech_recognition as sr\n+\n+# Перевод из формата mp3 в wav\n+\n+def mp3_to_wav(mp3_src, wav_src):\n+    sound = AudioSegment.from_mp3(mp3_src)\n+    sound.export(wav_src, format=""wav"")\n+\n+# mp3_to_wav(\'audio\\grob.mp3\', \'audio\\grob.wav\')\n+\n+\n+# Делим аудио (wav) на короткие части, сохраняем в папку tmp_chunks\n+\n+def divide_audio(audio_src, chunk_length_ms):\n+    audio = AudioSegment.from_file(audio_src, ""wav"") \n+    chunks = make_chunks(audio, chunk_length_ms)\n+    \n+    path = ""tmp_chunks""\n+    os.mkdir(path)\n+    \n+    chunks_src = []\n+    for i, chunk in enumerate(chunks):\n+        chunk_name = ""tmp_chunks\\chunk{0}.wav"".format(i)\n+        chunks_src.append(chunk_name)\n+        chunk.export(chunk_name, format=""wav"")\n+        \n+    return chunks_src\n+\n+\n+# Удаляем папку tmp_chunks с частями аудио\n+\n+def delete_tmp_chunks(chunks_src):\n+    for chunk in chunks_src:\n+        os.remove(chunk)\n+    os.rmdir(\'tmp_chunks\')\n+\n+# Соединяем распознанный по частям текст\n+\n+def combine_text(text_array):\n+    result_text = \'\'\n+    for text in text_array:\n+        result_text += text + \' \'\n+    return result_text\n+\n+\n+# ### Распознавание речи\n+\n+# Библиотека SpeechRecognition ( https://pypi.org/project/SpeechRecognition/ )\n+# \n+# Используется распознавание при помощи Google Speech Recognition\n+# \n+# Распознавание текста происходит непрерывно, даже когда речи нет, а есть только шум. Если распознать какие-то слова не удается, программа выдает ошибку. Поэтому нужно делить аудио на короткие части и распознавать их отдельно.\n+# \n+# Список языковых кодов для Google Speech Recognition: https://www.science.co.il/language/Locale-codes.php\n+\n+\n+# Распознавание без учета шума\n+\n+def recognize(audio_src):\n+    # делим аудио на части\n+    chunks_src = divide_audio(audio_src, 3000)\n+    # распознаем каждую часть\n+    text_array = []\n+    for chunk_src in chunks_src:\n+        a = sr.AudioFile(chunk_src)\n+        with a as source:\n+            audio = r.record(source)\n+        try:\n+            text_array.append(r.recognize_google(audio, language=\'ru\'))\n+        except:\n+            pass\n+    # объединяем распознанные тексты\n+    text = combine_text(text_array)\n+    # удаляем ненужные файлы\n+    delete_tmp_chunks(chunks_src)\n+    \n+    return text\n+\n+\n+#print(recognize(\'audio/vsyo_idet_po_planu.wav\'))\n+\n+\n+# Распознавание с учетом шума: уровень шума определяется автоматически\n+\n+def recognize_with_noise(audio_src):\n+    # делим аудио на части\n+    chunks_src = divide_audio(audio_src, 3000)\n+    # распознаем каждую часть\n+    text_array = []\n+    for chunk_src in chunks_src:\n+        a = sr.AudioFile(chunk_src)\n+        #print(\'analyzing \', chunk_src)\n+        with a as source:\n+            r.adjust_for_ambient_noise(source) # учитываем шум\n+            audio = r.record(source)\n+        try:\n+            text_chunk = r.recognize_google(audio, language=\'ru\')\n+            text_array.append(text_chunk)\n+            #print(text_chunk)\n+        except:\n+            pass\n+    # объединяем распознанные тексты\n+    text = combine_text(text_array)\n+    # удаляем ненужные файлы\n+    delete_tmp_chunks(chunks_src)\n+    \n+    return text\n+\n+#print(recognize_with_noise(\'audio/vsyo_idet_po_planu.wav\'))\n+\n+# итоговая функция распознавания\n+\n+def mp3_to_txt():\n+\n+    r = sr.Recognizer()\n+    mp3_to_wav(\'audio/grob.mp3\', \'audio/grob.wav\')\n+    recognize_with_noise(\'audio/grob.wav\')\n'}, {'new_path': 'TextFromPicture.py', 'diff': '@@ -1,4 +1,6 @@\n-# Import libraries \n+# -*- coding: utf-8 -*-\n+\n+# Import libraries\n # from PIL import Image хуже распознается\n import cv2 \n import pytesseract \n'}, {'new_path': 'get-video-stream.py', 'diff': ""@@ -0,0 +1,35 @@\n+#!/usr/bin/env python\n+from __future__ import unicode_literals, print_function\n+import argparse\n+import ffmpeg\n+import sys\n+\n+\n+parser = argparse.ArgumentParser(description='Generate video thumbnail')\n+parser.add_argument('in_filename', help='Input filename')\n+parser.add_argument('out_filename', help='Output filename')\n+parser.add_argument(\n+    '--time', type=int, default=0.1, help='Time offset')\n+parser.add_argument(\n+    '--width', type=int, default=120,\n+    help='Width of output thumbnail (height automatically determined by aspect ratio)')\n+\n+\n+def generate_thumbnail(in_filename, out_filename, time, width):\n+    try:\n+        (\n+            ffmpeg\n+            .input(in_filename, ss=time)\n+            .filter('scale', width, -1)\n+            .output(out_filename, vframes=1)\n+            .overwrite_output()\n+            .run(capture_stdout=True, capture_stderr=True)\n+        )\n+    except ffmpeg.Error as e:\n+        print(e.stderr.decode(), file=sys.stderr)\n+        sys.exit(1)\n+\n+\n+if __name__ == '__main__':\n+    args = parser.parse_args()\n+    generate_thumbnail(args.in_filename, args.out_filename, args.time, args.width)\n\\ No newline at end of file\n""}, {'new_path': 'main.py', 'diff': '@@ -1,4 +1,5 @@\n # -*- coding: utf-8 -*-\n+\n from test import Hello\n import argparse\n import sys\n'}, {'new_path': 'requirements.txt', 'diff': '@@ -1 +1,5 @@\n-\n+SpeechRecognition==3.8.1\n+ffmpeg==1.4\n+pdf2image==1.12.1\n+pydub==0.23.1\n+pytesseract==0.3.2\n'}]"
10,155,16cb79d9,"TextFromPicture recognizes text and saves it in .txt file. File is taken
from local directory for now
",1,83,0,"[{'new_path': 'TextFromPicture.py', 'diff': '@@ -0,0 +1,83 @@\n+# Import libraries \n+# from PIL import Image хуже распознается\n+import cv2 \n+import pytesseract \n+import sys \n+from pdf2image import convert_from_path \n+import os \n+\n+# Path of the pdf \n+PDF_file = ""FileName.pdf""\n+\n+\'\'\' \n+Part #1 : Converting PDF to images \n+\'\'\'\n+\n+# Store all the pages of the PDF in a variable \n+pages = convert_from_path(PDF_file, 500) \n+\n+# Counter to store images of each page of PDF to image \n+image_counter = 1\n+\n+# Iterate through all the pages stored above \n+for page in pages: \n+\n+\t# Declaring filename for each page of PDF as JPG \n+\t# For each page, filename will be: \n+\t# PDF page 1 -> page_1.jpg \n+\t# PDF page 2 -> page_2.jpg \n+\t# PDF page 3 -> page_3.jpg \n+\t# .... \n+\t# PDF page n -> page_n.jpg \n+\tfilename = ""page_""+str(image_counter)+"".jpg""\n+\t\n+\t# Save the image of the page in system \n+\tpage.save(filename, \'JPEG\') \n+\n+\t# Increment the counter to update filename \n+\timage_counter = image_counter + 1\n+\n+\'\'\' \n+Part #2 - Recognizing text from the images using OCR \n+\'\'\'\n+# Variable to get count of total number of pages \n+filelimit = image_counter-1\n+\n+# Creating a text file to write the output \n+outfile = ""out_text.txt""\n+\n+# Open the file in append mode so that \n+# All contents of all images are added to the same file \n+f = open(outfile, ""a"") \n+\n+# Iterate from 1 to total number of pages \n+for i in range(1, filelimit + 1): \n+\n+\t# Set filename to recognize text from \n+\t# Again, these files will be: \n+\t# page_1.jpg \n+\t# page_2.jpg \n+\t# .... \n+\t# page_n.jpg \n+\tfilename = ""page_""+str(i)+"".jpg""\n+\t\t\n+\t# Recognize the text as string in image using pytesserct \n+\ttext = str(((pytesseract.image_to_string(cv2.imread(filename),lang = \'eng+rus\')))) \n+\n+\t# The recognized text is stored in variable text \n+\t# Any string processing may be applied on text \n+\t# Here, basic formatting has been done: \n+\t# In many PDFs, at line ending, if a word can\'t \n+\t# be written fully, a \'hyphen\' is added. \n+\t# The rest of the word is written in the next line \n+\t# Eg: This is a sample text this word here GeeksF- \n+\t# orGeeks is half on first line, remaining on next. \n+\t# To remove this, we replace every \'-\\n\' to \'\'. \n+\ttext = text.replace(\'-\\n\', \'\')\t \n+\n+\t# Finally, write the processed text to the file. \n+\t#with open(fname, ""w"", encoding=""utf-8"") as f:\n+\tf.write(text) \n+\n+# Close the file after writing all the text. \n+f.close() \n'}]"
11,155,f40272f7,"Added video-stream getter
",2,37,0,"[{'new_path': '.gitattributes', 'diff': '@@ -0,0 +1,2 @@\n+# Auto detect text files and perform LF normalization\n+* text=auto\n'}, {'new_path': 'get-video-stream.py', 'diff': ""@@ -0,0 +1,35 @@\n+#!/usr/bin/env python\n+from __future__ import unicode_literals, print_function\n+import argparse\n+import ffmpeg\n+import sys\n+\n+\n+parser = argparse.ArgumentParser(description='Generate video thumbnail')\n+parser.add_argument('in_filename', help='Input filename')\n+parser.add_argument('out_filename', help='Output filename')\n+parser.add_argument(\n+    '--time', type=int, default=0.1, help='Time offset')\n+parser.add_argument(\n+    '--width', type=int, default=120,\n+    help='Width of output thumbnail (height automatically determined by aspect ratio)')\n+\n+\n+def generate_thumbnail(in_filename, out_filename, time, width):\n+    try:\n+        (\n+            ffmpeg\n+            .input(in_filename, ss=time)\n+            .filter('scale', width, -1)\n+            .output(out_filename, vframes=1)\n+            .overwrite_output()\n+            .run(capture_stdout=True, capture_stderr=True)\n+        )\n+    except ffmpeg.Error as e:\n+        print(e.stderr.decode(), file=sys.stderr)\n+        sys.exit(1)\n+\n+\n+if __name__ == '__main__':\n+    args = parser.parse_args()\n+    generate_thumbnail(args.in_filename, args.out_filename, args.time, args.width)\n\\ No newline at end of file\n""}]"
12,155,d18e36d9,"TextFromPicture takes pdf from local directory, recognizes text and rites it to .txt file",1,83,0,"[{'new_path': 'TextFromPicture.py', 'diff': '@@ -0,0 +1,83 @@\n+# Import libraries \n+# from PIL import Image хуже распознается\n+import cv2 \n+import pytesseract \n+import sys \n+from pdf2image import convert_from_path \n+import os \n+\n+# Path of the pdf \n+PDF_file = ""FileName.pdf""\n+\n+\'\'\' \n+Part #1 : Converting PDF to images \n+\'\'\'\n+\n+# Store all the pages of the PDF in a variable \n+pages = convert_from_path(PDF_file, 500) \n+\n+# Counter to store images of each page of PDF to image \n+image_counter = 1\n+\n+# Iterate through all the pages stored above \n+for page in pages: \n+\n+\t# Declaring filename for each page of PDF as JPG \n+\t# For each page, filename will be: \n+\t# PDF page 1 -> page_1.jpg \n+\t# PDF page 2 -> page_2.jpg \n+\t# PDF page 3 -> page_3.jpg \n+\t# .... \n+\t# PDF page n -> page_n.jpg \n+\tfilename = ""page_""+str(image_counter)+"".jpg""\n+\t\n+\t# Save the image of the page in system \n+\tpage.save(filename, \'JPEG\') \n+\n+\t# Increment the counter to update filename \n+\timage_counter = image_counter + 1\n+\n+\'\'\' \n+Part #2 - Recognizing text from the images using OCR \n+\'\'\'\n+# Variable to get count of total number of pages \n+filelimit = image_counter-1\n+\n+# Creating a text file to write the output \n+outfile = ""out_text.txt""\n+\n+# Open the file in append mode so that \n+# All contents of all images are added to the same file \n+f = open(outfile, ""a"") \n+\n+# Iterate from 1 to total number of pages \n+for i in range(1, filelimit + 1): \n+\n+\t# Set filename to recognize text from \n+\t# Again, these files will be: \n+\t# page_1.jpg \n+\t# page_2.jpg \n+\t# .... \n+\t# page_n.jpg \n+\tfilename = ""page_""+str(i)+"".jpg""\n+\t\t\n+\t# Recognize the text as string in image using pytesserct \n+\ttext = str(((pytesseract.image_to_string(cv2.imread(filename),lang = \'eng+rus\')))) \n+\n+\t# The recognized text is stored in variable text \n+\t# Any string processing may be applied on text \n+\t# Here, basic formatting has been done: \n+\t# In many PDFs, at line ending, if a word can\'t \n+\t# be written fully, a \'hyphen\' is added. \n+\t# The rest of the word is written in the next line \n+\t# Eg: This is a sample text this word here GeeksF- \n+\t# orGeeks is half on first line, remaining on next. \n+\t# To remove this, we replace every \'-\\n\' to \'\'. \n+\ttext = text.replace(\'-\\n\', \'\')\t \n+\n+\t# Finally, write the processed text to the file. \n+\t#with open(fname, ""w"", encoding=""utf-8"") as f:\n+\tf.write(text) \n+\n+# Close the file after writing all the text. \n+f.close() \n'}]"
